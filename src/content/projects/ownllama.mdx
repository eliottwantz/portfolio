---
title: "OwnLlama"
slug: "ownllama"
description: OwnLlama is your personal local AI assistant, leveraging Llama3 and Ollama. It allows simple chat and also RAG chat with your own documents.
publishDate: "2024-09-01"
isFeatured: true
tags: ["AI", "SvelteKit", "TypeScript", "Ollama", "Python", "Langchain", "Tailwind CSS"]
seo:
  image:
    src: "/plock/readme.png"
    alt: "Project preview"
---

import ProjectCarousel from "@/components/ProjectCarousel.svelte";

**Project Overview:**
OwnLlama is your personal local AI assistant, leveraging any open source LLM from Ollama. It allows simple chat and also RAG chat with your own documents.

<div class="project-carousel-shell">
  <ProjectCarousel client:load images={[{ src: "/ownllama/demo.gif", alt: "Demo of OwnLlama" }]} />
</div>

## Objectives

1. Have a fully free and local AI assistant.
2. Understand your own documents thanks to Retrieval Augmented Generation.
3. Provide a simple chat interface.
4. Get a better understanding of the Llama3 model.
5. Learn how to create an AI chatbot.

## Features

1. **Local**

- OwnLlama is fully local. This means it runs on your computer and never sends any data to the internet. This is possible thanks to the [**Ollama**](https://ollama.com/) project. Now you might get why I named it "OwnLlama".

2. **RAG**

- I wanted to create a simple chatbot that could understand my documents and provide answers based on my questions. This meant that I was able to provide my own sources of knowledge to the chatbot. This is possible with **Retrieval Augmented Generation** (RAG). To simplify the process, I used the [**Langchain**](https://python.langchain.com/) library.

## Architecture

Plock is self-contained and can be deployed in any environment supporting Docker. Under the hood, it is a Sveltekit app with a python backend. The Sveltekit app communicates with the Python backend over a REST API. For real-time chat completion, the Python backend returns **Server Sent Events** (SSE) response.

## Technology Stack

- [Sveltekit](https://kit.svelte.dev)
- [Tailwindcss](https://tailwindcss.com/)
- [Shadcn-Svelte](https://www.shadcn-svelte.com/)
- [FastAPI](https://fastapi.tiangolo.com/)
- [SQLModel](https://sqlmodel.tiangolo.com/)
- [Langchain](https://python.langchain.com/)
- [Ollama](https://ollama.com/)
- [Python](https://www.python.org/)
- [Docker](https://www.docker.com/)

## Final Words

I created this project to learn AI development using Large Language Models (LLMs). I first used the TypeScript LangChain library to keep everything in one language. It was all working well, but I decided to write the backend in Python to also use the Python library since it is mainly used in these LLMs/AI/ML apps nowadays.

## Links

- [Github](https://github.com/eliottwantz/OwnLlama)
