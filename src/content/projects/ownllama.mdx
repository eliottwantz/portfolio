---
title: 'OwnLlama'
description: OwnLlama is your personal local AI assistant, leveraging Llama3 and Ollama. It allows simple chat and also RAG chat with your own documents.
publishDate: 'September 1 2024'
isFeatured: true
tags: ['AI', 'Sveltekit', 'TypeScript', 'Ollama', 'Python', 'Langchain', 'Tailwindcss']
seo:
  image:
    src: '/plock/readme.png'
    alt: Project preview
---

import * as Card from '@/components/ui/card/index';
import { Image } from 'astro:assets';
import demoGif from '@/assets/ownllama/demo.gif';

**Project Overview:**
OwnLlama is your personal local AI assistant, leveraging any open source LLM from Ollama. It allows simple chat and also RAG chat with your own documents.

<div class="flex flex-col justify-center mb-10">
  <Card.Root>
    <Card.Content class="overflow-hidden flex aspect-square items-center justify-center">
      <Image src={demoGif} alt="Demo of OwnLlama" />
    </Card.Content>
    <small class="pl-6 text-sm text-muted-foreground">Demo of OwnLlama</small>
  </Card.Root>
</div>

## Objectives

1. Have a fully free and local AI assistant.
2. Understand your own documents thanks to Retrieval Augmented Generation.
3. Provide a simple chat interface.
4. Get a better understanding of the Llama3 model.
5. Learn how to create an AI chatbot.

## Features

1. **Local**

- OwnLlama is fully local. This means it runs on your computer and never sends any data to the internet. This is possible thanks to the [**Ollama**](https://ollama.com/) project. Now you might get why I named it "OwnLlama".

2. **RAG**

- I wanted to create a simple chatbot that could understand my documents and provide answers based on my questions. This meant that I was able to provide my own sources of knowledge to the chatbot. This is possible with **Retrieval Augmented Generation** (RAG). To simplify the process, I used the [**Langchain**](https://python.langchain.com/) library.

## Architecture

Plock is self-contained and can be deployed in any environment supporting Docker. Under the hood, it is a Sveltekit app with a python backend. The Sveltekit app communicates with the Python backend over a REST API. For real-time chat completion, the Python backend returns **Server Sent Events** (SSE) response.

## Technology Stack

- [Sveltekit](https://kit.svelte.dev)
- [Tailwindcss](https://tailwindcss.com/)
- [Shadcn-Svelte](https://www.shadcn-svelte.com/)
- [FastAPI](https://fastapi.tiangolo.com/)
- [SQLModel](https://sqlmodel.tiangolo.com/)
- [Langchain](https://python.langchain.com/)
- [Ollama](https://ollama.com/)
- [Python](https://www.python.org/)
- [Docker](https://www.docker.com/)

## Final Words

I created this project to learn AI development using Large Language Models (LLMs). I first used the TypeScript LangChain library to keep everything in one language. It was all working well, but I decided to write the backend in Python to also use the Python library since it is mainly used in these LLMs/AI/ML apps nowadays.
